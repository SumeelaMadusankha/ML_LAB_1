# -*- coding: utf-8 -*-
"""190376N_label2 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xXcsVOqRRGZMJkMrhSj6Ax18_WnKUVMO
"""

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import RobustScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load the data
train_data = pd.read_csv('train.csv')
valid_data = pd.read_csv('valid.csv')
test_data = pd.read_csv('test.csv')

#train data filtered according to the label 2 not na values
train_data = train_data[train_data['label_2'].notna()]#this is because of some values are NAN in label 2
X_train = train_data.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)
y_label2_train = train_data['label_2']

#fill NAN in valid data label 2 with median
valid_data['label_2'].fillna(valid_data['label_2'].median(), inplace=True)
X_valid = valid_data.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)
y_valid_label2 = valid_data['label_2']

#test data
X_test = test_data.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)

# Initialize models with  hyperparameters
model_label2 = KNeighborsRegressor(n_neighbors=256)

# Train models
model_label2.fit(X_train, y_label2_train)

# Predictions
pred_label2 = model_label2.predict(X_valid)
pred_label2_test = model_label2.predict(X_test)

mse = mean_squared_error(y_valid_label2, pred_label2)
r2s = r2_score(y_valid_label2, pred_label2)

print(f"Metrics for before feature engineering:")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R2 Score: {r2s:.2f}")
print("\n")

output_label2 = pd.DataFrame(index=range(750))
output_label2['Predicted labels before feature engineering'] = np.nan

# Fill the available rows with predicted values
output_label2.loc[:len(pred_label2_test)-1, 'Predicted labels before feature engineering'] = pred_label2_test

"""Applying Feature Engineering techniques"""

sc = RobustScaler()

X_train_scaled = sc.fit_transform(X_train)
X_valid_scaled = sc.transform(X_valid)
X_test_scaled = sc.transform(X_test)

# Calculate the variance threshold
desired_variance = 0.92  # Set the desired explained variance
pca = PCA(n_components=desired_variance, svd_solver='full')
X_train_pca = pca.fit_transform(X_train_scaled)
X_valid_pca =pca.transform(X_valid_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Get the number of components selected based on the variance threshold
n_components = pca.n_components_

model_label2.fit(X_train_pca,y_label2_train)
pred_label2= model_label2.predict(X_valid_pca)
pred_label2_test = model_label2.predict(X_test_pca)

mse = mean_squared_error(y_valid_label2, pred_label2)
r2s = r2_score(y_valid_label2, pred_label2)

print(f"Metrics for after feature engineering:")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R2 Score: {r2s:.2f}")
print("\n")


output_label2['Predicted labels after feature engineering']=pred_label2_test
output_label2['No of new features']=n_components
# Add PCA components to the DataFrame
for i in range(n_components):
    component_name = f'new_feature_{i+1}'
    output_label2[component_name] = X_test_pca[:, i]

# Save the output DataFrame to a CSV file
output_label2.to_csv('190376N_lable_2.csv', index=False)