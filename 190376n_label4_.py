# -*- coding: utf-8 -*-
"""190376N_label4 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/165qmd_RQLn-p1f4S50P1VEOZn2KFpDRA
"""

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import RobustScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score,recall_score,precision_score

# Load the data
train_data = pd.read_csv('train.csv')
valid_data = pd.read_csv('valid.csv')
test_data = pd.read_csv('test.csv')

# Drop the columns that are not required
X_train = train_data.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)
y_label4_train = train_data['label_4']

X_valid = valid_data.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)
y_valid_label_4 = valid_data['label_4']

X_test = test_data.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)
# Initialize an SVM model
model_label4 = SVC(kernel='linear', class_weight={6: 0.3}, random_state=42)

# Train models
model_label4.fit(X_train, y_label4_train)

# Predictions
pred_label4 = model_label4.predict(X_valid)
pred_label4_test = model_label4.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_valid_label_4, pred_label4)
precision = precision_score(y_valid_label_4, pred_label4, average='weighted', zero_division=1)
recall = recall_score(y_valid_label_4, pred_label4, average='weighted')

print(f"Metrics for label 4 before feature engineering:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print("\n")

output_label4 = pd.DataFrame(index=range(750))
output_label4['Predicted labels before feature engineering'] = np.nan

# Fill the available rows with predicted values
output_label4.loc[:len(pred_label4_test)-1, 'Predicted labels before feature engineering'] = pred_label4_test

"""Applying Feature Engineering techniques to reduce the features count"""

sc = RobustScaler()

X_train_scaled = sc.fit_transform(X_train)
X_valid_scaled = sc.transform(X_valid)
X_test_scaled = sc.transform(X_test)
# Calculate the variance threshold
desired_variance = 0.99 # Set the desired explained variance
pca = PCA(n_components=desired_variance, svd_solver='full')
X_train_pca = pca.fit_transform(X_train_scaled)
X_valid_pca =pca.transform(X_valid_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Get the number of components selected based on the variance threshold
n_components = pca.n_components_

model_label4.fit(X_train_pca,y_label4_train)
pred_label4= model_label4.predict(X_valid_pca)
pred_label4_test = model_label4.predict(X_test_pca)

 # Calculate metrics for classification evaluation on validation data
accuracy = accuracy_score(y_valid_label_4, pred_label4)
precision = precision_score(y_valid_label_4, pred_label4, average='weighted', zero_division=1)
recall = recall_score(y_valid_label_4, pred_label4, average='weighted')

print(f"Metrics for label 4 on after feature engineering:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print("\n")

output_label4['No of new features']=n_components
# Add PCA components to the DataFrame
for i in range(n_components):
    component_name = f'new_feature_{i+1}'
    output_label4[component_name] = X_test_pca[:, i]

# Save the output DataFrame to a CSV file
output_label4.to_csv('190376N_lable_4.csv', index=False)